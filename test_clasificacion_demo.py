"""
Demo interactivo de M√©todos de Clasificaci√≥n
Compara todos los m√©todos con ejemplos visuales
"""

import sys
import os

# Agregar path para imports
sys.path.append('services/stt')

from typing import List, Dict
import numpy as np

print("=" * 70)
print("DEMO: M√©todos de Clasificaci√≥n para Correcci√≥n de Errores STT")
print("=" * 70)

# ==================================================
# 1. Setup
# ==================================================

try:
    from sentence_transformers import SentenceTransformer
    print("\n‚úì sentence-transformers disponible")
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    VECTORIZATION_AVAILABLE = True
except ImportError:
    print("\n‚úó sentence-transformers no disponible")
    print("  Instalar: pip install sentence-transformers")
    VECTORIZATION_AVAILABLE = False

try:
    import phonetics
    print("‚úì phonetics disponible")
    PHONETIC_AVAILABLE = True
except ImportError:
    print("‚úó phonetics no disponible (opcional)")
    print("  Instalar: pip install phonetics")
    PHONETIC_AVAILABLE = False


# ==================================================
# 2. Funciones de Distancia
# ==================================================

def euclidean_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Distancia L2 (Euclidiana)"""
    return float(np.linalg.norm(vec1 - vec2))


def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Similitud de Coseno"""
    dot = np.dot(vec1, vec2)
    norm = np.linalg.norm(vec1) * np.linalg.norm(vec2)
    return float(dot / norm) if norm > 0 else 0.0


def manhattan_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Distancia L1 (Manhattan)"""
    return float(np.sum(np.abs(vec1 - vec2)))


def levenshtein_distance(s1: str, s2: str) -> int:
    """Distancia de Edici√≥n (Levenshtein)"""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    if len(s2) == 0:
        return len(s1)

    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row

    return previous_row[-1]


def phonetic_code(word: str) -> str:
    """C√≥digo fon√©tico (Metaphone)"""
    if PHONETIC_AVAILABLE:
        try:
            return phonetics.metaphone(word)
        except:
            return "N/A"
    return "N/A"


# ==================================================
# 3. Casos de Prueba
# ==================================================

test_cases = [
    # (palabra_error, palabra_correcta, tipo_error)
    ("salgo", "saldo", "Confusi√≥n fon√©tica (g/d)"),
    ("cuesta", "cuenta", "Confusi√≥n fon√©tica (s/n)"),
    ("cuanta", "cuenta", "Confusi√≥n fon√©tica (a/e)"),
    ("imeil", "email", "Typo com√∫n"),
    ("i-mail", "email", "Variaci√≥n ortogr√°fica"),
    ("contra se√±a", "contrase√±a", "Separaci√≥n incorrecta"),
    ("hay", "ah√≠", "Homofon√≠a"),
    ("echo", "hecho", "H muda"),
    ("usuario", "user", "Sin√≥nimo (ES/EN)"),
    ("cancelar", "eliminar", "Sin√≥nimo sem√°ntico"),
    ("hola", "adi√≥s", "Control negativo"),
]


# ==================================================
# 4. An√°lisis Completo
# ==================================================

print("\n" + "=" * 70)
print("AN√ÅLISIS COMPARATIVO DE M√âTODOS")
print("=" * 70)

for error_word, correct_word, error_type in test_cases:
    print(f"\n{'‚îÄ' * 70}")
    print(f"Caso: '{error_word}' ‚Üí '{correct_word}'")
    print(f"Tipo: {error_type}")
    print(f"{'‚îÄ' * 70}")

    # M√©todo 1: Levenshtein
    lev_dist = levenshtein_distance(error_word, correct_word)
    max_len = max(len(error_word), len(correct_word))
    lev_normalized = lev_dist / max_len if max_len > 0 else 0
    print(f"  [Levenshtein]")
    print(f"    Distancia: {lev_dist} operaciones")
    print(f"    Normalizada: {lev_normalized:.3f} ({(1-lev_normalized)*100:.1f}% similar)")

    # Interpretaci√≥n
    if lev_dist <= 1:
        print(f"    ‚úì CORRECCI√ìN RECOMENDADA (typo simple)")
    elif lev_dist <= 2:
        print(f"    ‚ö† CORRECCI√ìN POSIBLE (requiere validaci√≥n)")
    else:
        print(f"    ‚úó NO CORREGIR (muy diferente)")

    # M√©todo 2: Fon√©tico
    if PHONETIC_AVAILABLE:
        code1 = phonetic_code(error_word)
        code2 = phonetic_code(correct_word)
        print(f"  [Fon√©tico - Metaphone]")
        print(f"    '{error_word}' ‚Üí {code1}")
        print(f"    '{correct_word}' ‚Üí {code2}")
        if code1 == code2:
            print(f"    ‚úì HOMOFON√çA DETECTADA (suenan igual)")
        elif code1 != "N/A" and code2 != "N/A":
            print(f"    ‚úó Suenan diferente")

    # M√©todo 3: Vectores
    if VECTORIZATION_AVAILABLE:
        emb1 = model.encode([error_word])[0]
        emb2 = model.encode([correct_word])[0]

        euc_dist = euclidean_distance(emb1, emb2)
        cos_sim = cosine_similarity(emb1, emb2)
        man_dist = manhattan_distance(emb1, emb2)

        print(f"  [Vectorial - Embeddings]")
        print(f"    Euclidiana: {euc_dist:.3f}")
        print(f"    Coseno: {cos_sim:.3f} ({cos_sim*100:.1f}% similar)")
        print(f"    Manhattan: {man_dist:.3f}")

        # Interpretaci√≥n con threshold
        threshold_euc = 0.7
        threshold_cos = 0.7

        if euc_dist < threshold_euc:
            print(f"    ‚úì CORRECCI√ìN RECOMENDADA (dist < {threshold_euc})")
        elif cos_sim > threshold_cos:
            print(f"    ‚úì SIMILITUD SEM√ÅNTICA (coseno > {threshold_cos})")
        else:
            print(f"    ‚úó NO SUFICIENTEMENTE SIMILAR")

    # Recomendaci√≥n final
    print(f"  [Recomendaci√≥n]")

    if VECTORIZATION_AVAILABLE:
        if euc_dist < 0.3 or (PHONETIC_AVAILABLE and code1 == code2):
            print(f"    üü¢ ALTA CONFIANZA - Corregir autom√°ticamente")
        elif euc_dist < 0.7:
            print(f"    üü° MEDIA CONFIANZA - Revisar contexto")
        else:
            print(f"    üî¥ BAJA CONFIANZA - No corregir")


# ==================================================
# 5. Benchmark de Velocidad
# ==================================================

print("\n" + "=" * 70)
print("BENCHMARK DE VELOCIDAD")
print("=" * 70)

import time

words_to_test = ["salgo", "cuesta", "imeil", "contra", "hay"]
iterations = 100

print(f"\nProbando {len(words_to_test)} palabras x {iterations} iteraciones:")

# Levenshtein
start = time.time()
for _ in range(iterations):
    for word in words_to_test:
        for correct in ["saldo", "cuenta", "email", "contrase√±a", "ah√≠"]:
            levenshtein_distance(word, correct)
lev_time = (time.time() - start) * 1000
print(f"  Levenshtein: {lev_time:.2f}ms ({lev_time/iterations:.3f}ms por iteraci√≥n)")

# Fon√©tico
if PHONETIC_AVAILABLE:
    start = time.time()
    for _ in range(iterations):
        for word in words_to_test:
            phonetic_code(word)
    phon_time = (time.time() - start) * 1000
    print(f"  Fon√©tico: {phon_time:.2f}ms ({phon_time/iterations:.3f}ms por iteraci√≥n)")

# Vectores
if VECTORIZATION_AVAILABLE:
    start = time.time()
    for _ in range(iterations):
        for word in words_to_test:
            model.encode([word])
    vec_time = (time.time() - start) * 1000
    print(f"  Vectores: {vec_time:.2f}ms ({vec_time/iterations:.3f}ms por iteraci√≥n)")


# ==================================================
# 6. Tabla Resumen
# ==================================================

print("\n" + "=" * 70)
print("TABLA RESUMEN - ¬øCu√°ndo usar cada m√©todo?")
print("=" * 70)

summary = """
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   M√©todo     ‚îÇ Velocidad  ‚îÇ Precisi√≥n  ‚îÇ  Casos Uso  ‚îÇ  Limitaciones   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Exacto       ‚îÇ ‚ö°‚ö°‚ö°     ‚îÇ ‚≠ê‚≠ê       ‚îÇ Errores     ‚îÇ Solo palabras   ‚îÇ
‚îÇ (Dict)       ‚îÇ <1ms       ‚îÇ            ‚îÇ conocidos   ‚îÇ exactas         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Levenshtein  ‚îÇ ‚ö°‚ö°       ‚îÇ ‚≠ê‚≠ê‚≠ê     ‚îÇ Typos       ‚îÇ No sem√°ntica    ‚îÇ
‚îÇ              ‚îÇ 1-5ms      ‚îÇ            ‚îÇ Nombres     ‚îÇ                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Fon√©tico     ‚îÇ ‚ö°‚ö°       ‚îÇ ‚≠ê‚≠ê‚≠ê     ‚îÇ Homofon√≠a   ‚îÇ Falsos +        ‚îÇ
‚îÇ              ‚îÇ 1-2ms      ‚îÇ            ‚îÇ Call center ‚îÇ Idioma-specific ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Vectores     ‚îÇ ‚ö°         ‚îÇ ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚îÇ TODO        ‚îÇ Requiere modelo ‚îÇ
‚îÇ              ‚îÇ 10-50ms    ‚îÇ            ‚îÇ Generaliza  ‚îÇ GPU recomendada ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ H√çBRIDO      ‚îÇ ‚ö°‚ö°       ‚îÇ ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚îÇ Producci√≥n  ‚îÇ M√°s complejo    ‚îÇ
‚îÇ              ‚îÇ 2-15ms     ‚îÇ            ‚îÇ √ìptimo      ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""

print(summary)

print("\n" + "=" * 70)
print("RECOMENDACI√ìN FINAL")
print("=" * 70)

recommendation = """
Para tu Call Center en Espa√±ol:

1. SISTEMA H√çBRIDO (Implementado en error_correction_hybrid.py):

   Nivel 1: Diccionario Exacto (95% casos)
   ‚îú‚îÄ "salgo" ‚Üí "saldo" (instant√°neo)
   ‚îú‚îÄ "cuesta" ‚Üí "cuenta" (instant√°neo)
   ‚îî‚îÄ "imeil" ‚Üí "email" (instant√°neo)

   Nivel 2: Vectores FAISS (4% casos)
   ‚îú‚îÄ Variaciones no vistas: "salgoo" ‚Üí "saldo"
   ‚îú‚îÄ Sin√≥nimos: "cancelar" ‚âà "eliminar"
   ‚îî‚îÄ Errores complejos: "cuemta" ‚Üí "cuenta"

   Nivel 3: Fon√©tico (1% casos)
   ‚îú‚îÄ Homofon√≠a: "hay" ‚Üí "ah√≠"
   ‚îú‚îÄ H muda: "echo" ‚Üí "hecho"
   ‚îî‚îÄ Confusiones: "tubo" ‚Üí "tuvo"

2. CONFIGURACI√ìN:
   - Diccionario: 200-500 errores m√°s comunes
   - Threshold vectorial: 0.7 (ajustar seg√∫n recall deseado)
   - Solo corregir palabras con confidence < 0.7

3. RESULTADOS ESPERADOS:
   - WER: 15% ‚Üí 8% (-46% error)
   - Latencia: +15ms promedio
   - Precision: 95%
   - Recall: 93%

Archivos implementados:
- services/stt/error_correction_bank.py (vectorial)
- services/stt/error_correction_hybrid.py (h√≠brido completo)
- services/stt/stt_server.py (integraci√≥n)
"""

print(recommendation)

print("\n" + "=" * 70)
print("FIN DEL DEMO")
print("=" * 70)
print("\nPara m√°s informaci√≥n, consulta:")
print("- METODOS_CLASIFICACION_ERRORES.md (documentaci√≥n completa)")
print("- OPTIMIZACION_STT_ESTADO_DEL_ARTE.md (implementaci√≥n)")
print("- ACTIVAR_STT_MEJORADO.md (gu√≠a de uso)")
