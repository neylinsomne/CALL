"""
Voice Evolver - Stage 2
========================
Random walk algorithm that evolves Kokoro voice tensors to match
a target voice generated by F5-TTS.

Inspired by KVoiceWalk. Key steps:
1. Load existing Kokoro voices and optionally interpolate for starting points
2. Select best starting voice
3. Random walk: mutate → synthesize → score → keep if better
4. Save best evolved tensor

Mutation: base_tensor + randn_like(base) * std * diversity
"""

import json
import random
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
from loguru import logger
from tqdm import tqdm

from config import (
    WALK_STEPS, DIVERSITY_RANGE, EARLY_TERMINATION_RATIO,
    CLIP_TENSORS, EVOLVED_DIR, INTERPOLATED_DIR, SAMPLE_RATE,
    KOKORO_LANG, KOKORO_MODEL, STYLES,
)
from fitness_scorer import FitnessScorer
from voice_interpolator import VoiceInterpolator


class SpeechGenerator:
    """
    Wrapper around Kokoro TTS for voice synthesis during evolution.
    """

    def __init__(self, lang_code: str = KOKORO_LANG, model_id: str = KOKORO_MODEL):
        from kokoro import KPipeline
        self.pipeline = KPipeline(lang_code=lang_code, model_id=model_id)
        self.sample_rate = SAMPLE_RATE
        logger.info(f"Kokoro pipeline initialized: lang={lang_code}, model={model_id}")

    def generate(
        self,
        text: str,
        voice: torch.Tensor,
        speed: float = 1.0,
    ) -> Optional[np.ndarray]:
        """
        Synthesize text using a voice tensor.
        Returns audio as numpy float32 array.
        """
        try:
            audio_chunks = []
            for _, _, audio_chunk in self.pipeline(text, voice=voice, speed=speed):
                if audio_chunk is not None:
                    audio_chunks.append(audio_chunk)

            if not audio_chunks:
                return None

            return np.concatenate(audio_chunks).astype(np.float32)
        except Exception as e:
            logger.debug(f"Synthesis failed: {e}")
            return None


class VoiceEvolver:
    """
    Evolves Kokoro voice tensors using a random walk algorithm.
    """

    def __init__(
        self,
        target_audio_path: str,
        style: str = "neutral",
        walk_steps: int = WALK_STEPS,
        use_interpolation: bool = True,
        device: str = "cpu",
    ):
        """
        Args:
            target_audio_path: Path to the F5-TTS reference audio for this style.
            style: Style name (for logging and output naming).
            walk_steps: Number of random walk iterations.
            use_interpolation: Whether to use interpolation for starting points.
            device: PyTorch device.
        """
        self.target_audio_path = target_audio_path
        self.style = style
        self.walk_steps = walk_steps
        self.use_interpolation = use_interpolation
        self.device = device

        # Initialize components
        self.speech_gen = SpeechGenerator()
        self.scorer = FitnessScorer(target_audio_path)

        # Voice tensor statistics (computed from loaded voices)
        self.voice_stats: Optional[Dict] = None

        # Test phrases for scoring (two different texts for self-similarity)
        style_config = STYLES.get(style, STYLES["neutral"])
        phrases = style_config["reference_phrases"]
        self.test_text_a = phrases[0] if phrases else "Buenos dias, en que puedo ayudarle."
        self.test_text_b = phrases[1] if len(phrases) > 1 else "Gracias por llamar a nuestro servicio."

    def find_starting_voice(self) -> Tuple[torch.Tensor, float, str]:
        """
        Find the best starting voice for the random walk.
        Uses interpolation search if enabled, otherwise evaluates existing voices.
        """
        interpolator = VoiceInterpolator(
            scorer=self.scorer,
            speech_gen=self.speech_gen,
        )

        num_loaded = interpolator.load_voices()
        if num_loaded == 0:
            raise RuntimeError("No Kokoro voices found. Cannot start evolution.")

        # Save voice statistics for mutation scaling
        self.voice_stats = interpolator.get_statistics()

        if self.use_interpolation and num_loaded >= 2:
            logger.info("Starting interpolation search...")
            results = interpolator.interpolate_search(self.test_text_a)
        else:
            logger.info("Evaluating existing voices...")
            results = interpolator.evaluate_voices(self.test_text_a)

        if not results:
            raise RuntimeError("All voice evaluations failed.")

        best_name, best_score, best_tensor = results[0]
        logger.info(f"Starting voice: '{best_name}' with score {best_score:.2f}")
        return best_tensor, best_score, best_name

    def mutate(self, base_tensor: torch.Tensor, diversity: float) -> torch.Tensor:
        """
        Create a mutated version of a voice tensor.

        Mutation = base + randn_like(base) * std * diversity

        Args:
            base_tensor: Current best voice tensor.
            diversity: Scale factor for mutation (0.01-0.15 typical).

        Returns:
            Mutated tensor.
        """
        noise = torch.randn_like(base_tensor)

        if self.voice_stats and self.voice_stats["std"] is not None:
            std = self.voice_stats["std"].to(base_tensor.device)
            scaled_noise = noise * std * diversity
        else:
            # Fallback: use tensor's own statistics
            scaled_noise = noise * base_tensor.std() * diversity

        mutated = base_tensor + scaled_noise

        if CLIP_TENSORS and self.voice_stats:
            min_vals = self.voice_stats["min"]
            max_vals = self.voice_stats["max"]
            if min_vals is not None and max_vals is not None:
                mutated = torch.clamp(
                    mutated,
                    min=min_vals.to(mutated.device),
                    max=max_vals.to(mutated.device),
                )

        return mutated

    def evolve(
        self,
        output_dir: Path = EVOLVED_DIR,
        save_interval: int = 100,
    ) -> Tuple[torch.Tensor, float]:
        """
        Run the random walk evolution.

        Returns:
            Tuple of (best_tensor, best_score).
        """
        output_dir.mkdir(parents=True, exist_ok=True)
        run_log_path = output_dir / f"{self.style}_evolution_log.json"

        # Find starting voice
        best_tensor, best_score, start_name = self.find_starting_voice()

        logger.info(
            f"\n{'='*60}\n"
            f"Starting evolution for style '{self.style}'\n"
            f"  Starting voice: {start_name}\n"
            f"  Starting score: {best_score:.2f}\n"
            f"  Walk steps: {self.walk_steps}\n"
            f"{'='*60}"
        )

        # Evolution log
        log_entries = [{
            "step": 0,
            "score": best_score,
            "source": start_name,
            "diversity": 0.0,
            "timestamp": time.time(),
        }]

        improvements = 0
        skipped = 0
        start_time = time.time()

        for step in tqdm(range(1, self.walk_steps + 1), desc=f"Evolving '{self.style}'"):
            # Random diversity within range
            diversity = random.uniform(*DIVERSITY_RANGE)

            # Mutate
            candidate = self.mutate(best_tensor, diversity)

            # Generate audio with candidate voice
            audio_a = self.speech_gen.generate(self.test_text_a, candidate)
            if audio_a is None:
                skipped += 1
                continue

            # Early termination: quick similarity check
            quick_score = self.scorer._target_similarity(audio_a, SAMPLE_RATE)
            if quick_score < self.scorer.weights["target_similarity"] * (best_score / 100) * EARLY_TERMINATION_RATIO:
                skipped += 1
                continue

            # Full scoring (with self-similarity)
            audio_b = self.speech_gen.generate(self.test_text_b, candidate)
            result = self.scorer.score(audio_a, audio_b)
            candidate_score = result["overall"]

            if candidate_score > best_score:
                improvements += 1
                improvement_pct = ((candidate_score - best_score) / best_score) * 100
                best_tensor = candidate
                best_score = candidate_score

                logger.info(
                    f"  Step {step}: NEW BEST {best_score:.2f} "
                    f"(+{improvement_pct:.2f}%, diversity={diversity:.3f}, "
                    f"target={result['target_similarity']:.3f}, "
                    f"self={result['self_similarity']:.3f})"
                )

                log_entries.append({
                    "step": step,
                    "score": best_score,
                    "diversity": diversity,
                    "target_similarity": result["target_similarity"],
                    "self_similarity": result["self_similarity"],
                    "feature_similarity": result["feature_similarity"],
                    "timestamp": time.time(),
                })

                # Save improved tensor
                save_path = output_dir / f"{self.style}.pt"
                torch.save(best_tensor, save_path)

            # Periodic save of log
            if step % save_interval == 0:
                elapsed = time.time() - start_time
                rate = step / elapsed if elapsed > 0 else 0
                logger.info(
                    f"  Step {step}/{self.walk_steps}: "
                    f"best={best_score:.2f}, improvements={improvements}, "
                    f"skipped={skipped}, rate={rate:.1f} steps/s"
                )
                _save_log(run_log_path, log_entries)

        # Final save
        elapsed = time.time() - start_time
        final_path = output_dir / f"{self.style}.pt"
        torch.save(best_tensor, final_path)

        log_entries.append({
            "step": "final",
            "score": best_score,
            "total_steps": self.walk_steps,
            "improvements": improvements,
            "skipped": skipped,
            "elapsed_seconds": elapsed,
            "timestamp": time.time(),
        })
        _save_log(run_log_path, log_entries)

        logger.info(
            f"\n{'='*60}\n"
            f"Evolution complete for style '{self.style}'\n"
            f"  Final score: {best_score:.2f}\n"
            f"  Improvements: {improvements}\n"
            f"  Skipped: {skipped}\n"
            f"  Elapsed: {elapsed:.1f}s ({elapsed/60:.1f}min)\n"
            f"  Saved to: {final_path}\n"
            f"{'='*60}"
        )

        return best_tensor, best_score


def _save_log(path: Path, entries: list):
    """Save evolution log to JSON."""
    path.write_text(json.dumps(entries, indent=2, default=str))


async def run_stage(
    reference_dir: Path,
    output_dir: Path = EVOLVED_DIR,
    styles: Optional[List[str]] = None,
    walk_steps: int = WALK_STEPS,
    use_interpolation: bool = True,
) -> Dict[str, Path]:
    """
    Entry point for Stage 2: evolve voices for all styles.

    Args:
        reference_dir: Directory with target audio per style.
        output_dir: Where to save evolved tensors.
        styles: Specific styles to evolve (None = all).
        walk_steps: Number of random walk steps per style.
        use_interpolation: Use interpolation for starting points.

    Returns:
        Dict of style → path to evolved .pt tensor.
    """
    target_styles = styles or list(STYLES.keys())
    results = {}

    for style in target_styles:
        target_audio = reference_dir / style / "target.wav"
        if not target_audio.exists():
            logger.error(f"No target audio for style '{style}': {target_audio}")
            continue

        evolver = VoiceEvolver(
            target_audio_path=str(target_audio),
            style=style,
            walk_steps=walk_steps,
            use_interpolation=use_interpolation,
        )

        tensor, score = evolver.evolve(output_dir)
        results[style] = output_dir / f"{style}.pt"
        logger.info(f"Style '{style}' evolved: score={score:.2f}")

    return results


if __name__ == "__main__":
    import argparse
    import asyncio

    parser = argparse.ArgumentParser(description="Stage 2: Evolve Kokoro voice tensors")
    parser.add_argument("--reference-dir", type=Path, required=True, help="Dir with target audio per style")
    parser.add_argument("--output-dir", type=Path, default=EVOLVED_DIR)
    parser.add_argument("--styles", nargs="+", help="Specific styles to evolve")
    parser.add_argument("--steps", type=int, default=WALK_STEPS, help="Random walk steps")
    parser.add_argument("--no-interpolation", action="store_true")
    args = parser.parse_args()

    results = asyncio.run(run_stage(
        reference_dir=args.reference_dir,
        output_dir=args.output_dir,
        styles=args.styles,
        walk_steps=args.steps,
        use_interpolation=not args.no_interpolation,
    ))
    print(f"\nEvolved voices: {list(results.keys())}")
